
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>coverforest._forest &#8212; coverforest 0.1.dev77+g07d1adf.d20250118 documentation</title>



  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>

  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />

  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=7170f6c5"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/coverforest/_forest';</script>
    <link rel="icon" href="../../_static/coverforest.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="0.1.dev77+g07d1adf" />
  </head>


  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">



  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  <div id="pst-scroll-pixel-helper"></div>

  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>


  <dialog id="pst-search-dialog">

<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>


    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">





      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">



  <div class="sidebar-header-items sidebar-primary__section">




  </div>

    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">





<a class="navbar-brand logo" href="../../index.html">










    <img src="../../_static/coverforest_96.png" class="logo__image only-light" alt="coverforest 0.1.dev77+g07d1adf.d20250118 documentation - Home"/>
    <img src="../../_static/coverforest_96.png" class="logo__image only-dark pst-js-only" alt="coverforest 0.1.dev77+g07d1adf.d20250118 documentation - Home"/>


</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../classification/classification_gridsearch.html">Searching <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span> using scikit-learn’s <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../classification/classification_pipeline.html">Conformal prediction using CV+ in scikit-learn pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../regression/regression_pipeline.html">Conformal regression using Jackknife+-after-bootstrap in scikit-learn pipeline</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api.html">API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../generated/coverforest.CoverForestClassifier.html">CoverForestClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/coverforest.CoverForestRegressor.html">CoverForestRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/coverforest.metrics.average_interval_length_loss.html">coverforest.metrics.average_interval_length_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/coverforest.metrics.average_set_size_loss.html">coverforest.metrics.average_set_size_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/coverforest.metrics.classification_coverage_score.html">coverforest.metrics.classification_coverage_score</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generated/coverforest.metrics.regression_coverage_score.html">coverforest.metrics.regression_coverage_score</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>


  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>

      <main id="main-content" class="bd-main" role="main">



<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">

              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">

    <div class="header-article-items__start">

        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>

    </div>


    <div class="header-article-items__end">

        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/donlapark/coverforest" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>


<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>

    </div>

</div>
</div>



<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">

        </div>
    </div>
</div>



<div id="searchbox"></div>
                <article class="bd-article">

  <h1>Source code for coverforest._forest</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Conformal Random Forest methods.</span>

<span class="sd">Those methods include four different implementations of random forests that use</span>
<span class="sd">conformal prediction methods to output set predictions.</span>

<span class="sd">The module structure is the following:</span>

<span class="sd">The ``CoverForestClassifier`` and ``CoverForestRegressor`` classes each provide three</span>
<span class="sd">methods random forest classifiers that output prediction sets and prediction</span>
<span class="sd">intervals using conformal prediction.</span>

<span class="sd">    - For classification, the prediction sets are obtained using adaptive</span>
<span class="sd">      prediction set (APS) proposed by Romano, Sesia &amp; Candès (2020). The code also</span>
<span class="sd">      provides regularization introduced by Angelopoulos, Bates, Malik &amp; Jordan</span>
<span class="sd">      (2021) to encourage smaller sets.</span>
<span class="sd">    - For regression, the prediction intervals are obtained using the Jackknife+ and</span>
<span class="sd">      CV+ on the residuals proposed by Barber, Candès, Ramdas &amp; Tibshirani (2021).</span>

<span class="sd">The method, specified in the ``method`` parameter, includes:</span>

<span class="sd">    - ``method=cv``:   Random Forest with CV+ for prediction sets/intervals</span>
<span class="sd">    - ``method=bootstrap``:   Random Forest with Jackknife+-after-Bootstrap for</span>
<span class="sd">      prediction sets/intervals</span>
<span class="sd">    - ``method=split``:   Random Forest with split adaptive prediction set</span>
<span class="sd">      (APS) for prediction sets/intervals</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: Gilles Louppe &lt;g.louppe@gmail.com&gt;</span>
<span class="c1">#          Brian Holt &lt;bdholt1@gmail.com&gt;</span>
<span class="c1">#          Joly Arnaud &lt;arnaud.v.joly@gmail.com&gt;</span>
<span class="c1">#          Fares Hedayati &lt;fares.hedayati@gmail.com&gt;</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numbers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Integral</span><span class="p">,</span> <span class="n">Real</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warnings</span><span class="w"> </span><span class="kn">import</span> <span class="n">catch_warnings</span><span class="p">,</span> <span class="n">simplefilter</span><span class="p">,</span> <span class="n">warn</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.sparse</span><span class="w"> </span><span class="kn">import</span> <span class="n">issparse</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">_fit_context</span><span class="p">,</span> <span class="n">is_classifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble._base</span><span class="w"> </span><span class="kn">import</span> <span class="n">_partition_estimators</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble._forest</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">BaseForest</span><span class="p">,</span>
    <span class="n">ForestClassifier</span><span class="p">,</span>
    <span class="n">ForestRegressor</span><span class="p">,</span>
    <span class="n">_get_n_samples_bootstrap</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree._tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DOUBLE</span><span class="p">,</span> <span class="n">DTYPE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">compute_sample_weight</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils._param_validation</span><span class="w"> </span><span class="kn">import</span> <span class="n">Interval</span><span class="p">,</span> <span class="n">StrOptions</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils._tags</span><span class="w"> </span><span class="kn">import</span> <span class="n">ClassifierTags</span><span class="p">,</span> <span class="n">RegressorTags</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils.multiclass</span><span class="w"> </span><span class="kn">import</span> <span class="n">type_of_target</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils.validation</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_check_sample_weight</span><span class="p">,</span>
    <span class="n">_check_y</span><span class="p">,</span>
    <span class="n">check_consistent_length</span><span class="p">,</span>
    <span class="n">check_is_fitted</span><span class="p">,</span>
    <span class="n">check_random_state</span><span class="p">,</span>
    <span class="n">validate_data</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">._fast_random_forest</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">BaseFastForest</span><span class="p">,</span>
    <span class="n">FastRandomForestClassifier</span><span class="p">,</span>
    <span class="n">FastRandomForestRegressor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">._giqs</span><span class="w"> </span><span class="kn">import</span> <span class="n">_compute_predictions_split</span><span class="p">,</span> <span class="n">_compute_test_giqs_cv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">average_interval_length_loss</span><span class="p">,</span>
    <span class="n">average_set_size_loss</span><span class="p">,</span>
    <span class="n">classification_coverage_score</span><span class="p">,</span>
    <span class="n">regression_coverage_score</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">MAX_INT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_generate_sample_indices</span><span class="p">(</span>
    <span class="n">random_state</span><span class="p">,</span> <span class="n">kfold_indices</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples_bootstrap</span><span class="p">,</span> <span class="n">method</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate sample indices for building trees.</span>

<span class="sd">    Private function used to generate sample indices for individual trees in parallel</span>
<span class="sd">    forest construction.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    random_state : int, RandomState instance or None.</span>
<span class="sd">        The random number generator instance.</span>
<span class="sd">    kfold_indices : list of tuples of ndarrays or None</span>
<span class="sd">        Pairs of train-test indices obtained from k-fold cross-validation.</span>
<span class="sd">    k : int or None</span>
<span class="sd">        Current fold number.</span>
<span class="sd">    tree_idx : int or None</span>
<span class="sd">        Index of the current tree.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Total number of samples.</span>
<span class="sd">    n_samples_bootstrap : int</span>
<span class="sd">        Number of samples to draw for bootstrap.</span>
<span class="sd">    method : {&#39;cv&#39;, &#39;bootstrap&#39;, &#39;split&#39;}</span>
<span class="sd">        The method used for generating sample indices.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    sample_indices : ndarray of shape (n_samples_bootstrap,)</span>
<span class="sd">        The generated sample indices for building the tree.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;cv&quot;</span><span class="p">:</span>
        <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">kfold_indices</span><span class="p">[</span><span class="n">tree_idx</span> <span class="o">%</span> <span class="n">k</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">random_instance</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">random_instance</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples_bootstrap</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">sample_indices</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_parallel_build_trees</span><span class="p">(</span>
    <span class="n">tree</span><span class="p">,</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">method</span><span class="p">,</span>
    <span class="n">kfold_indices</span><span class="p">,</span>
    <span class="n">k</span><span class="p">,</span>
    <span class="n">sample_weight</span><span class="p">,</span>
    <span class="n">tree_idx</span><span class="p">,</span>
    <span class="n">n_trees</span><span class="p">,</span>
    <span class="n">oob_mat</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_samples_bootstrap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_classes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fit a single tree in parallel on a subsample obtained via `method`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    tree : BaseDecisionTree</span>
<span class="sd">        The decision tree instance to be built.</span>
<span class="sd">    X : ndarray of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples.</span>
<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        The target values.</span>
<span class="sd">    method : {&#39;cv&#39;, &#39;bootstrap&#39;, &#39;split&#39;}</span>
<span class="sd">        Subsampling method used for conformal predictions.</span>
<span class="sd">    kfold_indices : list of tuples of ndarrays</span>
<span class="sd">        Used when `method=&#39;cv&#39;`. Pairs of train-test indices obtained from</span>
<span class="sd">        k-fold cross-validation.</span>
<span class="sd">    k : int</span>
<span class="sd">        Used when `method=&#39;cv&#39;`. Current fold number.</span>
<span class="sd">    sample_weight : array-like of shape (n_samples,) or None</span>
<span class="sd">        Sample weights.</span>
<span class="sd">    tree_idx : int</span>
<span class="sd">        Index of the current tree.</span>
<span class="sd">    n_trees : int</span>
<span class="sd">        Total number of trees to be built.</span>
<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Controls the verbosity of the tree building process.</span>
<span class="sd">    class_weight : {&quot;balanced&quot;, &quot;balanced_subsample&quot;}, dict or list of dicts, \</span>
<span class="sd">            default=None</span>
<span class="sd">        Class weights.</span>
<span class="sd">    n_samples : int or None, default=None</span>
<span class="sd">        Number of samples in the dataset.</span>
<span class="sd">    n_samples_bootstrap : int or None, default=None</span>
<span class="sd">        Used when `method=&#39;bootstrap&#39;`. Number of samples to draw for bootstrap.</span>
<span class="sd">    n_classes : int or None, default=None</span>
<span class="sd">        Number of classes in the dataset.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    tree : BaseDecisionTree</span>
<span class="sd">        The fitted decision tree.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;building tree </span><span class="si">%d</span><span class="s2"> of </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tree_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_trees</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">curr_sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">curr_sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="n">indices</span> <span class="o">=</span> <span class="n">_generate_sample_indices</span><span class="p">(</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">kfold_indices</span><span class="p">,</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">tree_idx</span><span class="p">,</span>
        <span class="n">n_samples</span><span class="p">,</span>
        <span class="n">n_samples_bootstrap</span><span class="p">,</span>
        <span class="n">method</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">sample_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="n">oob_mat</span><span class="p">[:,</span> <span class="n">tree_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_counts</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">sample_counts</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="s2">&quot;n_classes_&quot;</span><span class="p">):</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="n">n_classes</span>

    <span class="k">if</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s2">&quot;subsample&quot;</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">catch_warnings</span><span class="p">():</span>
            <span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
            <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">compute_sample_weight</span><span class="p">(</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s2">&quot;balanced_subsample&quot;</span><span class="p">:</span>
        <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">compute_sample_weight</span><span class="p">(</span><span class="s2">&quot;balanced&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span>

    <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">curr_sample_weight</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tree</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_accumulate_prediction</span><span class="p">(</span><span class="n">predict</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Store the test predictions of each tree in an array.</span>

<span class="sd">    This method will be called in parallel.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ConformalClassifierMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mixin class for conformal classifiers in scikit-learn.</span>

<span class="sd">    This mixin defines the following functionality:</span>

<span class="sd">    - set estimator type to `&quot;classifier&quot;` through the `estimator_type` tag;</span>
<span class="sd">    - `score` method that evaluates both coverage and average set size;</span>
<span class="sd">    - enforce that `fit` requires `y` to be passed through the `requires_y` tag.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Conformal classifiers output prediction sets C(X) that satisfy:</span>

<span class="sd">        P(Y ∈ C(X)) ≥ 1 - α</span>

<span class="sd">    where:</span>

<span class="sd">    - α is the desired miscoverage rate</span>
<span class="sd">    - The probability is taken over the pair (X,Y)</span>
<span class="sd">    - This guarantee holds for any data distribution, with an assumption that</span>
<span class="sd">      the samples (X₁,Y₁), ... , (Xₙ,Yₙ) are i.i.d. Some methods require a</span>
<span class="sd">      weaker assumption that they are exchangable.</span>

<span class="sd">    The score method supports different metrics:</span>

<span class="sd">    - &#39;coverage&#39;: Empirical coverage probability</span>
<span class="sd">    - &#39;size&#39;: Average size of prediction sets</span>
<span class="sd">    - &#39;both&#39;: Returns both coverage and average size.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.base import BaseEstimator</span>
<span class="sd">    &gt;&gt;&gt; from coverforest import ConformalClassifierMixin</span>
<span class="sd">    &gt;&gt;&gt; # Mixin classes should always be on the left-hand side for a correct MRO</span>
<span class="sd">    &gt;&gt;&gt; class MyEstimator(ConformalClassifierMixin, BaseEstimator):</span>
<span class="sd">    ...     def __init__(self, *, param=1):</span>
<span class="sd">    ...         self.param = param</span>
<span class="sd">    ...     def fit(self, X, y):</span>
<span class="sd">    ...         self.is_fitted_ = True</span>
<span class="sd">    ...         self.n_classes_ = np.unique(y).shape[0]</span>
<span class="sd">    ...         return self</span>
<span class="sd">    ...     def predict(self, X):</span>
<span class="sd">    ...         return np.full(shape=(X.shape[0], self.n_classes_),</span>
<span class="sd">                             fill_value=self.param)</span>
<span class="sd">    &gt;&gt;&gt; estimator = MyEstimator(param=1)</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[1, 2], [2, 3], [3, 4]])</span>
<span class="sd">    &gt;&gt;&gt; y = np.array([1, 0, 1])</span>
<span class="sd">    &gt;&gt;&gt; estimator.fit(X, y).predict(X)</span>
<span class="sd">    array([[1, 1], [1, 1], [1, 1]])</span>
<span class="sd">    &gt;&gt;&gt; clf.score(X, y, alpha=0.05, scoring=&#39;both&#39;)</span>
<span class="sd">    (0.66, 2)...</span>


<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Yaniv Romano, Matteo Sesia, Emmanuel J. Candès. Classification with</span>
<span class="sd">           Valid and Adaptive Coverage. NeurIPS 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_estimator_type</span> <span class="o">=</span> <span class="s2">&quot;classifier&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;coverage&quot;</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate the prediction set on the given test data and labels.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            Test samples.</span>

<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            True labels for X.</span>

<span class="sd">        alpha : float, default=None</span>
<span class="sd">            The desired miscoverage rate. The method will construct prediction sets</span>
<span class="sd">            with approximately (1-alpha) coverage.</span>

<span class="sd">        scoring : {&#39;size&#39;, &#39;coverage&#39;, &#39;both&#39;}, default=&#39;size&#39;</span>
<span class="sd">            The scoring metric to use:</span>

<span class="sd">            - &#39;size&#39;: returns the average size of prediction sets</span>
<span class="sd">            - &#39;coverage&#39;: returns the empirical coverage (proportion of sets that</span>
<span class="sd">              contain true labels)</span>
<span class="sd">            - &#39;both&#39;: returns a tuple of (coverage, average_size)</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : float or tuple</span>
<span class="sd">            If scoring=&#39;size&#39;: returns average prediction set size (float)</span>
<span class="sd">            If scoring=&#39;coverage&#39;: returns empirical coverage (float)</span>
<span class="sd">            If scoring=&#39;both&#39;: returns (coverage, average_size) tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">_check_y</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">y_set</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">binary_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;size&quot;</span> <span class="ow">or</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;both&quot;</span><span class="p">:</span>
            <span class="n">avg_size</span> <span class="o">=</span> <span class="n">average_set_size_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_set</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;size&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">avg_size</span>
        <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;coverage&quot;</span> <span class="ow">or</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;both&quot;</span><span class="p">:</span>
            <span class="n">coverage</span> <span class="o">=</span> <span class="n">classification_coverage_score</span><span class="p">(</span>
                <span class="n">y</span><span class="p">,</span> <span class="n">y_set</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;coverage&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">coverage</span>
        <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;both&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">coverage</span><span class="p">,</span> <span class="n">avg_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`scoring` must be one of `coverage`, `size` or `both`.&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__sklearn_tags__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tags</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__sklearn_tags__</span><span class="p">()</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">estimator_type</span> <span class="o">=</span> <span class="s2">&quot;classifier&quot;</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">classifier_tags</span> <span class="o">=</span> <span class="n">ClassifierTags</span><span class="p">()</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">non_deterministic</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">input_tags</span><span class="o">.</span><span class="n">allow_nan</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">target_tags</span><span class="o">.</span><span class="n">required</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">target_tags</span><span class="o">.</span><span class="n">single_output</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">target_tags</span><span class="o">.</span><span class="n">multi_output</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">tags</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ConformalRegressorMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mixin class for conformal regressors in scikit-learn.</span>

<span class="sd">    This mixin defines the following functionality:</span>

<span class="sd">    - set estimator type to `&quot;regressor&quot;` through the `estimator_type` tag;</span>
<span class="sd">    - `score` method that evaluates both coverage and average interval length;</span>
<span class="sd">    - enforce that `fit` requires `y` to be passed through the `requires_y` tag.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Conformal regressors output prediction intervals [l(X), u(X)] that satisfy:</span>

<span class="sd">        P(l(X) ≤ Y ≤ u(X)) ≥ 1 - α</span>

<span class="sd">    where:</span>

<span class="sd">    - α is the desired miscoverage rate</span>
<span class="sd">    - The probability is taken over the pair (X,Y)</span>
<span class="sd">    - This guarantee holds for any data distribution with an assumption that</span>
<span class="sd">      the samples (X₁,Y₁), ... , (Xₙ,Yₙ) are i.i.d. Some methods require a</span>
<span class="sd">      weaker assumption that they are exchangable.</span>

<span class="sd">    The score method supports different metrics:</span>

<span class="sd">    - &#39;coverage&#39;: Empirical coverage probability</span>
<span class="sd">    - &#39;length&#39;: Average length of prediction intervals</span>
<span class="sd">    - &#39;both&#39;: Returns both coverage and length metrics</span>

<span class="sd">    The intervals can be constructed using the three methods:</span>
<span class="sd">    - Jackknife+</span>
<span class="sd">    - CV+</span>
<span class="sd">    - Jackknife+-after-Bootstrap</span>

<span class="sd">    Each method provides valid coverage guarantees.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.base import BaseEstimator</span>
<span class="sd">    &gt;&gt;&gt; from coverforest import ConformalClassifierMixin</span>
<span class="sd">    &gt;&gt;&gt; class MyConformalRegressor(ConformalRegressorMixin, BaseEstimator):</span>
<span class="sd">    ...     def predict(self, X, alpha=0.05):</span>
<span class="sd">    ...         n_samples = X.shape[0]</span>
<span class="sd">    ...         y_pred = np.zeros(n_samples)</span>
<span class="sd">    ...         intervals = np.column_stack([-np.ones(n_samples),</span>
<span class="sd">    ...                                     np.ones(n_samples)])</span>
<span class="sd">    ...         return y_pred, intervals</span>
<span class="sd">    &gt;&gt;&gt; reg = MyConformalRegressor()</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])</span>
<span class="sd">    &gt;&gt;&gt; y = np.array([0.5, 1.5])</span>
<span class="sd">    &gt;&gt;&gt; reg.score(X, y, alpha=0.05, scoring=&#39;both&#39;)</span>
<span class="sd">    (0.5, 2)...</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Rina Foygel Barber, Emmanuel J. Candès, Aaditya Ramdas, and</span>
<span class="sd">           Ryan J. Tibshirani. &quot;Predictive inference with the jackknife+.&quot;</span>
<span class="sd">           Ann. Statist., 49(1):486–507, (2021).</span>
<span class="sd">    .. [2] Byol Kim, Chen Xu, Rina Foygel Barber. Predictive inference is free</span>
<span class="sd">           with the jackknife+-after-bootstrap. NeurIPS 2020.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_estimator_type</span> <span class="o">=</span> <span class="s2">&quot;regressor&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;coverage&quot;</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate the prediction intervals on the given test data and labels.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            Test samples.</span>

<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            True labels for X.</span>

<span class="sd">        alpha : float, default=None</span>
<span class="sd">            The desired miscoverage rate. The method will construct prediction</span>
<span class="sd">            intervals with approximately (1-alpha) coverage.</span>

<span class="sd">        scoring : {&#39;length&#39;, &#39;coverage&#39;, &#39;both&#39;}, default=&#39;length&#39;</span>
<span class="sd">            The scoring metric to use:</span>

<span class="sd">            - &#39;length&#39;: returns the average length of prediction intervals</span>
<span class="sd">            - &#39;coverage&#39;: returns the empirical coverage (proportion of true values in</span>
<span class="sd">            intervals)</span>
<span class="sd">            - &#39;both&#39;: returns a tuple of (coverage, average_length)</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : float or tuple</span>
<span class="sd">            If scoring=&#39;length&#39;: returns average interval length (float)</span>
<span class="sd">            If scoring=&#39;coverage&#39;: returns empirical coverage (float)</span>
<span class="sd">            If scoring=&#39;both&#39;: returns (coverage, average_length) tuple</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">_check_y</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">y_intervals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;length&quot;</span> <span class="ow">or</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;both&quot;</span><span class="p">:</span>
            <span class="n">avg_length</span> <span class="o">=</span> <span class="n">average_interval_length_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_intervals</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;length&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">avg_length</span>
        <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;coverage&quot;</span> <span class="ow">or</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;both&quot;</span><span class="p">:</span>
            <span class="n">coverage</span> <span class="o">=</span> <span class="n">regression_coverage_score</span><span class="p">(</span>
                <span class="n">y</span><span class="p">,</span> <span class="n">y_intervals</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;coverage&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">coverage</span>
        <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s2">&quot;both&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">coverage</span><span class="p">,</span> <span class="n">avg_length</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`scoring` must be one of `&#39;coverage&#39;`, `&#39;length&#39;` or `&#39;both&#39;`.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__sklearn_tags__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tags</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__sklearn_tags__</span><span class="p">()</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">estimator_type</span> <span class="o">=</span> <span class="s2">&quot;regressor&quot;</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">regressor_tags</span> <span class="o">=</span> <span class="n">RegressorTags</span><span class="p">()</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">non_deterministic</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">input_tags</span><span class="o">.</span><span class="n">allow_nan</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">target_tags</span><span class="o">.</span><span class="n">required</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">target_tags</span><span class="o">.</span><span class="n">single_output</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">tags</span><span class="o">.</span><span class="n">target_tags</span><span class="o">.</span><span class="n">multi_output</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">tags</span>


<span class="k">class</span><span class="w"> </span><span class="nc">BaseConformalForest</span><span class="p">(</span><span class="n">BaseForest</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Base class for conformal forests of trees.</span>

<span class="sd">    This class extends scikit-learn&#39;s BaseForest to implement conformal</span>
<span class="sd">    prediction.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_parameter_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">BaseForest</span><span class="o">.</span><span class="n">_parameter_constraints</span><span class="p">,</span>
        <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;bootstrap&quot;</span><span class="p">,</span> <span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="s2">&quot;split&quot;</span><span class="p">})],</span>
        <span class="s2">&quot;cv&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">),</span> <span class="s2">&quot;cv_object&quot;</span><span class="p">],</span>
        <span class="s2">&quot;n_forests_per_fold&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;resample_n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;boolean&quot;</span><span class="p">],</span>
    <span class="p">}</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">estimator</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">method</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">n_forests_per_fold</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">resample_n_estimators</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_forests_per_fold</span> <span class="o">=</span> <span class="n">n_forests_per_fold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resample_n_estimators</span> <span class="o">=</span> <span class="n">resample_n_estimators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_check_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">expanded_class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validate or convert input data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The training input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csc_matrix``.</span>

<span class="sd">        y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted.</span>

<span class="sd">        expanded_class_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Expanded class weights computed from `class_weight` parameter.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The validated and converted feature matrix.</span>

<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            The validated target values.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,)</span>
<span class="sd">            The validated sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The model requires y to be passed, but the target y is None.&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;sparse multilabel-indicator for y is not supported.&quot;</span><span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">validate_data</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="n">y</span><span class="p">,</span>
                <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span>
                <span class="n">ensure_min_samples</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">ensure_all_finite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Pre-sort indices to avoid that each individual tree of the</span>
            <span class="c1"># ensemble sorts the indices.</span>
            <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># reshape is necessary to preserve the data contiguity against vs</span>
                <span class="c1"># [:, np.newaxis] that does not.</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">==</span> <span class="s2">&quot;poisson&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">y</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Some value(s) of y are negative which is &quot;</span>
                        <span class="s2">&quot;not allowed for Poisson regression.&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Sum of y is not strictly positive which &quot;</span>
                        <span class="s2">&quot;is necessary for Poisson regression.&quot;</span>
                    <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_fit_wrapper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">calib_size</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit the forest using either CV+ or split conformal method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The training input samples.</span>

<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            The target values.</span>

<span class="sd">        calib_size : float</span>
<span class="sd">            The proportion of samples to use for calibration in split conformal.</span>
<span class="sd">            Only used when method=&#39;split&#39;.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="s2">&quot;bootstrap&quot;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fit_cv</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fit_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">calib_size</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@_fit_context</span><span class="p">(</span><span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_fit_cv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; &quot;Build a forest of trees using CV+ or Jackknife+-after-bootstrap.</span>

<span class="sd">        This method will be called when calling the `fit` method with</span>
<span class="sd">        `method=&#39;cv&#39;` or `method=&#39;bootstrap&#39;`.</span>

<span class="sd">        - If `method=&#39;cv&#39;`, the data will be split into K folds and the trees</span>
<span class="sd">          will be fitted on K-1 folds.</span>
<span class="sd">        - If `method=&#39;bootstrap&#39;`, the trees will be fitted on bootstrap</span>
<span class="sd">        subsamples.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The training input samples. Internally, its dtype will be converted</span>
<span class="sd">            to ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csc_matrix``.</span>

<span class="sd">        y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>

<span class="sd">        cv : int or cross-validation generator</span>
<span class="sd">            Determines the cross-validation splitting strategy. If int,</span>
<span class="sd">            specifies the number of folds. See scikit-learn&#39;s model selection</span>
<span class="sd">            module for available cross-validation objects. Only used when</span>
<span class="sd">            `method=&#39;cv&#39;`.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y_class_weight</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">!=</span> <span class="n">DOUBLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">y</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">contiguous</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DOUBLE</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">expanded_class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">expanded_class_weight</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expanded_class_weight</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">y_</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;bootstrap&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples_bootstrap</span> <span class="o">=</span> <span class="n">_get_n_samples_bootstrap</span><span class="p">(</span>
                <span class="n">n_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_samples</span>
            <span class="p">)</span>
            <span class="n">binom_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples_bootstrap</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">resample_n_estimators</span><span class="p">:</span>
                <span class="n">n_estimators</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
                    <span class="mi">2</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">/</span> <span class="n">binom_prop</span><span class="p">,</span> <span class="n">binom_prop</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">n_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples_bootstrap</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_n_cv_folds</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cv</span><span class="p">,</span> <span class="n">Integral</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">cv</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;k-fold cross-validation requires at least one&quot;</span>
                        <span class="s2">&quot; train/test split by setting n_splits=2 or more,&quot;</span>
                        <span class="s2">&quot; got n_splits=</span><span class="si">{0}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cv</span><span class="p">)</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">cv</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The number of splits of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="si">}</span><span class="s2"> is too low. Set the&quot;</span>
                        <span class="s2">&quot; number of splits as the number of samples instead.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">cv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_n_cv_folds</span> <span class="o">=</span> <span class="n">cv</span>

                <span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span>
                    <span class="n">n_splits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_cv_folds</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span>
                <span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="n">cv</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cv</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_cv_folds</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="p">)</span>

            <span class="n">n_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_forests_per_fold</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_cv_folds</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples_bootstrap</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kfold_indices_</span> <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_estimator</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">!=</span> <span class="s2">&quot;bootstrap&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Out of bag estimation only available if method=oob&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimators_&quot;</span><span class="p">):</span>
            <span class="c1"># Free allocated memory, if any</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oob_matrix_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">n_more_estimators</span> <span class="o">=</span> <span class="n">n_estimators</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="c1"># Decapsulate classes_ attributes</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;classes_&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;n_classes_&quot;</span><span class="p">):</span>
            <span class="n">oob_pred_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">)</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">oob_pred_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">)</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">n_more_estimators</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;n_estimators=</span><span class="si">%d</span><span class="s2"> must be larger or equal to &quot;</span>
                <span class="s2">&quot;len(estimators_)=</span><span class="si">%d</span><span class="s2"> when warm_start==True&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="n">n_estimators</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">n_more_estimators</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Warm-start fitting without increasing n_estimators does not &quot;</span>
                <span class="s2">&quot;fit new trees.&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># We draw from the random state to get the random state we</span>
                <span class="c1"># would have got if we hadn&#39;t used a warm_start.</span>
                <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span>

            <span class="n">trees</span> <span class="o">=</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">append</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_more_estimators</span><span class="p">)</span>
            <span class="p">]</span>

            <span class="c1"># Parallel loop: we prefer the threading backend as the Cython code</span>
            <span class="c1"># for fitting the trees is internally releasing the Python GIL</span>
            <span class="c1"># making threading more efficient than multiprocessing in</span>
            <span class="c1"># that case. However, for joblib 0.12+ we respect any</span>
            <span class="c1"># parallel_backend contexts set at a higher level,</span>
            <span class="c1"># since correctness does not rely on using threads.</span>
            <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="n">X_csr</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">X_csr</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">trees</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span>
                <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">prefer</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span>
            <span class="p">)(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="n">_parallel_build_trees</span><span class="p">)(</span>
                    <span class="n">t</span><span class="p">,</span>
                    <span class="n">X</span><span class="p">,</span>
                    <span class="n">y</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kfold_indices_</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_n_cv_folds</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">i</span><span class="p">,</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">oob_matrix_</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                    <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span>
                    <span class="n">n_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="p">,</span>
                    <span class="n">n_samples_bootstrap</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples_bootstrap</span><span class="p">,</span>
                    <span class="n">n_classes</span><span class="o">=</span><span class="n">n_classes</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trees</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># Collect newly grown trees</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">trees</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">n_more_estimators</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;oob_score_&quot;</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">y_type</span> <span class="o">=</span> <span class="n">type_of_target</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">y_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;multiclass-multioutput&quot;</span><span class="p">,</span> <span class="s2">&quot;unknown&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The type of target cannot be used to compute OOB &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;estimates. Got </span><span class="si">{</span><span class="n">y_type</span><span class="si">}</span><span class="s2"> while only the following are &quot;</span>
                    <span class="s2">&quot;supported: continuous, binary. &quot;</span>
                <span class="p">)</span>

        <span class="c1"># calculate oob scores</span>
        <span class="n">oob_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">oob_pred_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="n">pred_func</span> <span class="o">=</span> <span class="s2">&quot;predict_proba&quot;</span> <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;predict&quot;</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tree</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">):</span>
            <span class="n">predict_func</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">pred_func</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">X_csr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">y_pred</span> <span class="o">=</span> <span class="n">predict_func</span><span class="p">(</span>
                    <span class="n">X_csr</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">oob_matrix_</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="p">:],</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">y_pred</span> <span class="o">=</span> <span class="n">predict_func</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">oob_matrix_</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="p">:],</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">oob_pred</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">oob_matrix_</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span> <span class="o">+=</span> <span class="n">y_pred</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_n_oob_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_matrix_</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_oob_pred</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">warn</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="s2">&quot;Some inputs do not have OOB scores. This probably means &quot;</span>
                    <span class="s2">&quot;too few trees were used to compute any reliable OOB &quot;</span>
                    <span class="s2">&quot;estimates.&quot;</span>
                <span class="p">),</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_oob_pred</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_oob_pred</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">oob_pred</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_oob_pred</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_pred_</span> <span class="o">=</span> <span class="n">oob_pred</span>

        <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_giqs_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_train_giqs</span><span class="p">(</span><span class="n">oob_pred</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@_fit_context</span><span class="p">(</span><span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_fit_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">calib_size</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build a forest of trees using split conformal prediction.</span>

<span class="sd">        This method will be called when calling the `fit` method with</span>
<span class="sd">        `method=&#39;split&#39;`.</span>

<span class="sd">        The training set will be split into a smaller training set and a</span>
<span class="sd">        calibration set. The model will be fitted on the former and calibration</span>
<span class="sd">        scores will be calculated on the latter.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The training input samples. Internally, its dtype will be converted</span>
<span class="sd">            to ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csc_matrix``.</span>

<span class="sd">        y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>

<span class="sd">        calib_size : float</span>
<span class="sd">            The proportion of samples to use for calibration. Should be in</span>
<span class="sd">            (0, 1).</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;feature_names_in_&quot;</span><span class="p">):</span>
            <span class="n">feature_names_in_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_names_in_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">feature_names_in_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">y</span><span class="p">,</span> <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y_class_weight</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">expanded_class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">expanded_class_weight</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expanded_class_weight</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">!=</span> <span class="n">DOUBLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">y</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">contiguous</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DOUBLE</span><span class="p">)</span>

        <span class="n">sample_weight_int</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">is_integer_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_weight_int</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">is_integer_weights</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight_int</span>
            <span class="n">rep_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">sample_weight</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">rep_indices</span><span class="p">]</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">rep_indices</span><span class="p">]</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

        <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span> <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_cal</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_cal</span><span class="p">,</span> <span class="n">sample_weight_train</span><span class="p">,</span> <span class="n">sample_weight_cal</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">train_test_split</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="n">y</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="p">,</span>
                <span class="n">test_size</span><span class="o">=</span><span class="n">calib_size</span><span class="p">,</span>
                <span class="n">stratify</span><span class="o">=</span><span class="n">stratify</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="n">BaseFastForest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight_train</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;oob_score_&quot;</span><span class="p">):</span>
            <span class="n">y_type</span> <span class="o">=</span> <span class="n">type_of_target</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">y_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;multiclass-multioutput&quot;</span><span class="p">,</span> <span class="s2">&quot;unknown&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The type of target cannot be used to compute OOB &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;estimates. Got </span><span class="si">{</span><span class="n">y_type</span><span class="si">}</span><span class="s2"> while only the following are &quot;</span>
                    <span class="s2">&quot;supported: continuous, continuous-multioutput, binary, &quot;</span>
                    <span class="s2">&quot;multiclass, multilabel-indicator.&quot;</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">feature_names_in_</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_pred_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_cal</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;classes_&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_pred_</span> <span class="o">=</span> <span class="n">FastRandomForestRegressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_cal</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">feature_names_in_</span> <span class="o">=</span> <span class="n">feature_names_in_</span>

        <span class="k">if</span> <span class="n">y_cal</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">y_cal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_cal</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_</span> <span class="o">=</span> <span class="n">y_cal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_giqs_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_train_giqs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">oob_pred_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@_fit_context</span><span class="p">(</span><span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_oob_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute out-of-bag predictions on the test set `X`.</span>

<span class="sd">        This method will be called during the prediction with `method=&#39;cv&#39;` or</span>
<span class="sd">        `method=&#39;bootstrap&#39;`. It provides each training point the average</span>
<span class="sd">        predictions (or probability predictions for classification) of its</span>
<span class="sd">        out-of-bag trees on `X`.</span>

<span class="sd">        The output of this method will be used to calculate the conformity</span>
<span class="sd">        scores of each point in `X`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            The input samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_pred : ndarray</span>
<span class="sd">            The predicted values.</span>

<span class="sd">        oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or /</span>
<span class="sd">                (n_samples, 1, n_outputs)</span>
<span class="sd">            The out-of-bag predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Prediction requires X to be in CSR format</span>
        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_estimators</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
        <span class="n">classifier</span> <span class="o">=</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">classifier</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;n_classes_&quot;</span><span class="p">):</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">classifier</span><span class="p">:</span>
            <span class="n">y_pred_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
            <span class="n">y_pred_all_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_pred_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,)</span>
            <span class="n">y_pred_all_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_pred_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_pred_all_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="n">n_jobs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_partition_estimators</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span>
        <span class="n">classifier</span> <span class="o">=</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">pred_func</span> <span class="o">=</span> <span class="s2">&quot;predict_proba&quot;</span> <span class="k">if</span> <span class="n">classifier</span> <span class="k">else</span> <span class="s2">&quot;predict&quot;</span>

        <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">require</span><span class="o">=</span><span class="s2">&quot;sharedmem&quot;</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_accumulate_prediction</span><span class="p">)(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">pred_func</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">y_pred_all</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">classifier</span><span class="p">:</span>
            <span class="n">oob_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_matrix_</span> <span class="o">@</span> <span class="n">y_pred_all</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">oob_pred</span> <span class="o">=</span> <span class="n">oob_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
            <span class="n">oob_pred</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_oob_pred</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">oob_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_matrix_</span> <span class="o">@</span> <span class="n">y_pred_all</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_oob_pred</span>

        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred_all</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">oob_pred</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_make_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Make and configure a copy of the base estimator.</span>

<span class="sd">        This is a modification of sklearn&#39;s `_make_estimator` method only when</span>
<span class="sd">        `method=&#39;cv&#39;` to fix the problem of model&#39;s parameters not being</span>
<span class="sd">        passed to the `FastRandomForest` sub-estimators correctly.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        append : bool, default=True</span>
<span class="sd">            If True, append the estimator to the list of estimators.</span>

<span class="sd">        random_state : int, RandomState instance or None, default=None</span>
<span class="sd">            Controls the random seed used to initialize the base estimator.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        estimator : object</span>
<span class="sd">            The configured base estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;cv&quot;</span><span class="p">:</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">)</span>
            <span class="n">estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">p</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_params</span><span class="p">})</span>

            <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
                <span class="n">to_set</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">get_params</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">)):</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;random_state&quot;</span> <span class="ow">or</span> <span class="n">key</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;__random_state&quot;</span><span class="p">):</span>
                        <span class="n">to_set</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">to_set</span><span class="p">:</span>
                    <span class="n">estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">to_set</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">append</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">append</span><span class="p">,</span> <span class="n">random_state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimator</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_validate_X_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validate X whenever one tries to predict, apply, predict_proba.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            The input samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            The validated input.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If sparse matrix has wrong dtype for indices/indptr.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">validate_data</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">X</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span>
            <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span>
            <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">ensure_all_finite</span><span class="o">=</span><span class="s2">&quot;allow-nan&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">intc</span> <span class="ow">or</span> <span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">intc</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No support for np.int64 index based sparse matrices&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ConformalForestClassifier</span><span class="p">(</span>
    <span class="n">ConformalClassifierMixin</span><span class="p">,</span> <span class="n">BaseConformalForest</span><span class="p">,</span> <span class="n">ForestClassifier</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for forest of conformal trees-based classifiers.</span>

<span class="sd">    This class extends scikit-learn&#39;s ForestClassifier to implement conformal</span>
<span class="sd">    prediction.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">estimator</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">method</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">k_init</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">lambda_init</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">repeat_params_search</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">allow_empty_sets</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">randomized</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">alpha_default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_forests_per_fold</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">resample_n_estimators</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
        <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
            <span class="n">n_forests_per_fold</span><span class="o">=</span><span class="n">n_forests_per_fold</span><span class="p">,</span>
            <span class="n">resample_n_estimators</span><span class="o">=</span><span class="n">resample_n_estimators</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">k_init</span> <span class="o">=</span> <span class="n">k_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_init</span> <span class="o">=</span> <span class="n">lambda_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">repeat_params_search</span> <span class="o">=</span> <span class="n">repeat_params_search</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">randomized</span> <span class="o">=</span> <span class="n">randomized</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allow_empty_sets</span> <span class="o">=</span> <span class="n">allow_empty_sets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_default</span> <span class="o">=</span> <span class="n">alpha_default</span>

    <span class="nd">@_fit_context</span><span class="p">(</span><span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">calib_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">valid_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit the conformal forest classifier.</span>

<span class="sd">        This method first finds optimal values for the regularization</span>
<span class="sd">        parameters k and lambda using the procedure from Angelopoulos et al.</span>
<span class="sd">        (2021), then fits the forest using either CV+,</span>
<span class="sd">        Jackknife+-after-Bootstrap, or split conformal prediction.</span>

<span class="sd">        The k parameter penalizes prediction sets containing more than k</span>
<span class="sd">        classes, while lambda controls the strength of this penalty. When</span>
<span class="sd">        `k_init=&#39;auto&#39;` and/or `lambda_init=&#39;auto&#39;`, these parameters are chosen</span>
<span class="sd">        automatically:</span>

<span class="sd">        - k is set to the (1-alpha)-quantile of the rank of true labels in the</span>
<span class="sd">          out-of-bag predictions</span>
<span class="sd">        - lambda is selected from `[0.001, 0.01, 0.1, 0.2, 0.5, 1]` using a</span>
<span class="sd">          held-out validation set to minimize prediction set size</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Training data.</span>

<span class="sd">        y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">            Target values (class labels).</span>

<span class="sd">        alpha : float, default=0.05</span>
<span class="sd">            Desired miscoverage rate used to search for the optimal `k` and</span>
<span class="sd">            `lambda`. The prediction sets will be constructed to contains the</span>
<span class="sd">            true label with probability at least 1-alpha.</span>

<span class="sd">            This parameter will not be used when both `k_init` and</span>
<span class="sd">            `lambda_init` are both numeric values.</span>

<span class="sd">        calib_size : float, default=0.3</span>
<span class="sd">            Used when method=&#39;split&#39;. The proportion of training samples to use</span>
<span class="sd">            for calibration.</span>

<span class="sd">        valid_size : float, default=0.3</span>
<span class="sd">            The proportion of samples to use for validation when searching for</span>
<span class="sd">            the optimal lambda parameter.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted estimator.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        .. [1] Anastasios Nikolas Angelopoulos, Stephen Bates,</span>
<span class="sd">               Michael I. Jordan &amp; Jitendra Malik, &quot;Uncertainty Sets for Image</span>
<span class="sd">               Classifiers using Conformal Prediction&quot;, ICLR 2021.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">search_k_and_lambda</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">calib_size</span><span class="p">,</span> <span class="n">valid_size</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting with k = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span><span class="si">}</span><span class="s2"> and lambda = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_fit_wrapper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">calib_size</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">search_k_and_lambda</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">calib_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">valid_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Search for optimal values of k and lambda parameters and store them as</span>
<span class="sd">        attributes `k_star_` and `lambda_star_`, respectively.</span>

<span class="sd">        The parameter search follows the procedure suggested by Angelopoulos,</span>
<span class="sd">        Bates, Jordan &amp; Jitendra Malik (2021):</span>

<span class="sd">        - `k_star_` is the (1-alpha)-quantile of the rank of true y in the</span>
<span class="sd">          out-of-bag predictions</span>
<span class="sd">        - `lambda_star_` is chosen from the candidates</span>
<span class="sd">          `[0.001, 0.01, 0.1, 0.2, 0.5, 1]` using a held-out validation set.</span>

<span class="sd">        The parameter search be performed only when constructing the model with</span>
<span class="sd">        `lambda_init=&#39;auto&#39;` and/or `k_init=&#39;auto&#39;`, otherwise they will be set</span>
<span class="sd">        to the provided values.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            The training input samples.</span>

<span class="sd">        y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">            The target values.</span>

<span class="sd">        alpha : float, default=0.05</span>
<span class="sd">            The desired miscoverage rate.</span>

<span class="sd">        calib_size : float, default=0.3</span>
<span class="sd">            Used when `method=&#39;split&#39;`. The proportion of training samples to</span>
<span class="sd">            use for split conformal prediction.</span>

<span class="sd">        valid_size : float, default=0.3</span>
<span class="sd">            The proportion of samples to use for validation of lambda.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        k_star : int</span>
<span class="sd">            The value of k obtained from the (1-alpha) of the observed ranks of</span>
<span class="sd">            the training targets.</span>

<span class="sd">        lambda_star : float</span>
<span class="sd">            The value of lambda obtained from the grid search cross-validation.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        .. [1] Anastasios Nikolas Angelopoulos, Stephen Bates,</span>
<span class="sd">               Michael I. Jordan &amp; Jitendra Malik, &quot;Uncertainty Sets for Image</span>
<span class="sd">               Classifiers using Conformal Prediction&quot;, ICLR 2021.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_init</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">repeat_params_search</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;k_star_&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_init</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_init</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">repeat_params_search</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;lambda_star_&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_init</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Searching regularization parameters...&quot;</span><span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

            <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
            <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">))</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fit_wrapper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">calib_size</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
                <span class="n">true_label_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_pred_</span><span class="p">[</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
                <span class="p">]</span>
                <span class="n">scores_compare</span> <span class="o">=</span> <span class="n">true_label_scores</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_pred_</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
                <span class="n">y_ranks</span> <span class="o">=</span> <span class="n">scores_compare</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
                <span class="n">k_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">y_ranks</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;higher&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span> <span class="o">=</span> <span class="n">k_star</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">sw1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">,</span>
                    <span class="n">y</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">test_size</span><span class="o">=</span><span class="n">valid_size</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                    <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">best_sum_size</span> <span class="o">=</span> <span class="n">MAX_INT</span>
                <span class="k">for</span> <span class="n">lambda_</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span> <span class="o">=</span> <span class="n">lambda_</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_fit_wrapper</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">calib_size</span><span class="p">,</span> <span class="n">sw1</span><span class="p">)</span>
                    <span class="n">_</span><span class="p">,</span> <span class="n">y_set_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">binary_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="n">sum_size</span> <span class="o">=</span> <span class="n">y_set_pred</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">sum_size</span> <span class="o">&lt;</span> <span class="n">best_sum_size</span><span class="p">:</span>
                        <span class="n">best_sum_size</span> <span class="o">=</span> <span class="n">sum_size</span>
                        <span class="n">lambda_star</span> <span class="o">=</span> <span class="n">lambda_</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span> <span class="o">=</span> <span class="n">lambda_star</span>

        <span class="n">k_star</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span>
        <span class="n">lambda_star</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span>

        <span class="k">return</span> <span class="n">k_star</span><span class="p">,</span> <span class="n">lambda_star</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_train_giqs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">oob_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the regularized generalized inverse quantile conformity</span>
<span class="sd">        scores (giqs) of the training samples.</span>

<span class="sd">        Given a training sample with its out-of-bag probability predictions</span>
<span class="sd">        sorted in descending order π̂₁ ≥ ... ≥ π̂ₗ, if the sample&#39;s true label is</span>
<span class="sd">        j, then its (non-regularized) giqs is obtained by first sampling U from</span>
<span class="sd">        Uniform[0, 1] and then calculate::</span>

<span class="sd">            π̂₁ + ... + π̂ⱼ₋₁ + U * π̂ⱼ.</span>

<span class="sd">        The regularized giqs is then obtained by adding a term that penalizes</span>
<span class="sd">        sets by how much their sizes are larger than k::</span>

<span class="sd">            π̂₁ + ... + π̂ⱼ₋₁ + U * π̂ⱼ + λ * max(0, j - k + 1).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        oob_pred : ndarray of shape (n_samples, n_classes, n_outputs)</span>
<span class="sd">            The out-of-bag predictions of the training samples.</span>

<span class="sd">        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">            The true target values of the training samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        train_giqs : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            The generalized inverse quantile scores of the training samples.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This method only supports `n_outputs == 1` at the moment.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        .. [1] Yaniv Romano, Matteo Sesia &amp; Emmanuel J. Candès, &quot;Classification</span>
<span class="sd">               with Valid and Adaptive Coverage&quot;, NeurIPS 2020.</span>
<span class="sd">        .. [2] Anastasios Nikolas Angelopoulos, Stephen Bates,</span>
<span class="sd">               Michael I. Jordan &amp; Jitendra Malik, &quot;Uncertainty Sets for Image</span>
<span class="sd">               Classifiers using Conformal Prediction&quot;, ICLR 2021.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">y_score</span> <span class="o">=</span> <span class="n">oob_pred</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span> <span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)]</span>
        <span class="n">pred_probs</span> <span class="o">=</span> <span class="n">oob_pred</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">y_compare</span> <span class="o">=</span> <span class="n">y_score</span> <span class="o">&lt;</span> <span class="n">pred_probs</span>
        <span class="n">y_rank</span> <span class="o">=</span> <span class="n">y_compare</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">tau_before_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_compare</span> <span class="o">*</span> <span class="n">pred_probs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="n">tau_before_y</span> <span class="o">+</span> <span class="n">y_score</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="n">penalty</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">penalty</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span> <span class="p">:]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">randomized</span><span class="p">:</span>
            <span class="n">train_giqs</span> <span class="o">=</span> <span class="n">tau</span> <span class="o">+</span> <span class="n">penalty</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">train_giqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tau</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
            <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">tau</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="c1"># Decide whether to keep U; y might be removed if y_rank == 0</span>
            <span class="n">zero_idx</span> <span class="o">=</span> <span class="n">y_rank</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">allow_empty_sets</span><span class="p">:</span>
                <span class="n">train_giqs</span><span class="p">[</span><span class="n">zero_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tau</span><span class="p">[</span><span class="n">zero_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">penalty</span><span class="p">[</span><span class="n">zero_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">train_giqs</span><span class="p">[</span><span class="n">zero_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">U</span><span class="p">[</span><span class="n">zero_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">tau</span><span class="p">[</span><span class="n">zero_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">penalty</span><span class="p">[</span><span class="n">zero_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="n">non_zero_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="o">~</span><span class="n">zero_idx</span><span class="p">]</span>
            <span class="n">sum_penalty</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                <span class="p">[</span><span class="n">penalty</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span> <span class="p">:</span> <span class="p">(</span><span class="n">y_rank</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">non_zero_idx</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">train_giqs</span><span class="p">[</span><span class="n">non_zero_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">U</span><span class="p">[</span><span class="n">non_zero_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">y_score</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">non_zero_idx</span><span class="p">]</span>
                <span class="o">+</span> <span class="n">tau_before_y</span><span class="p">[</span><span class="n">non_zero_idx</span><span class="p">]</span>
                <span class="o">+</span> <span class="n">sum_penalty</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">train_giqs</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_test_giqs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">oob_pred</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the regularized generalized inverse quantile comformity</span>
<span class="sd">        scores (giqs) of test samples.</span>

<span class="sd">        This method will be called during prediction when `method=&#39;cv&#39;`.</span>

<span class="sd">        Given a test sample with its out-of-bag probability predictions sorted</span>
<span class="sd">        in descending order π̂₁ ≥ ... ≥ π̂ₗ, the sample&#39;s (non-regularized) giqs</span>
<span class="sd">        for a label j is obtained by first sampling U from Uniform[0, 1] and</span>
<span class="sd">        then calculate::</span>

<span class="sd">            π̂₁ + ... + π̂ⱼ₋₁ + U * π̂ⱼ.</span>

<span class="sd">        The regularized giqs is then obtained by adding a term that penalizes</span>
<span class="sd">        prediction sets that are larger than k::</span>

<span class="sd">            π̂₁ + ... + π̂ⱼ₋₁ + U * π̂ⱼ + λ * max(0, j - k + 1).</span>

<span class="sd">        This method will be called during prediction when `method=&#39;cv&#39;` or</span>
<span class="sd">        `method=&#39;bootstrap&#39;`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        oob_pred : ndarray of shape (n_train, n_classes, n_test)</span>
<span class="sd">            The out-of-bag predictions of the test data.</span>

<span class="sd">        num_threads : int, default=4</span>
<span class="sd">            Number of threads to use for parallel computation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        test_giqs : ndarray of shape (n_train, n_classes, n_test)</span>
<span class="sd">            The generalized inverse quantile conformity scores for test data.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        .. [1] Yaniv Romano, Matteo Sesia &amp; Emmanuel J. Candès, &quot;Classification</span>
<span class="sd">               with Valid and Adaptive Coverage&quot;, NeurIPS 2020.</span>
<span class="sd">        .. [2] Anastasios Nikolas Angelopoulos, Stephen Bates,</span>
<span class="sd">               Michael I. Jordan &amp; Jitendra Malik, &quot;Uncertainty Sets for Image</span>
<span class="sd">               Classifiers using Conformal Prediction&quot;, ICLR 2021.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">test_giqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">oob_pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">get_state</span><span class="p">()[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">_compute_test_giqs_cv</span><span class="p">(</span>
            <span class="n">oob_pred</span><span class="p">,</span>
            <span class="n">test_giqs</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">randomized</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">allow_empty_sets</span><span class="p">,</span>
            <span class="n">num_threads</span><span class="p">,</span>
            <span class="n">seed</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">test_giqs</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_predict_from_giqs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct prediction sets from calibration set&#39;s generalized inverse</span>
<span class="sd">        quantile comformity scores (giqs). The construction follows Algorithm 3</span>
<span class="sd">        in Angelopoulos, Bates, Jordan &amp; Malik (2021).</span>

<span class="sd">        For each sample, given probability predictions π̂₁ ≥ ... ≥ π̂ₗ sorted</span>
<span class="sd">        in descending order, the method first finds the label L whose</span>
<span class="sd">        regularized cumulative probabilities is just above τ::</span>

<span class="sd">            τ ≤ π̂₁ + ... + π̂ʟ + λ * max(0, L - k + 1) ≤ τ + 1,</span>

<span class="sd">        where k is the regularization parameter.</span>

<span class="sd">        Define π̂ʟ&#39; = π̂ʟ + λ if L ≥ k, otherwise π̂&#39;ʟ = π̂ʟ. Sample  U from</span>
<span class="sd">        Uniform[0, 1]. The prediction set is {1, ... , L - 1} if</span>

<span class="sd">            π̂₁ + ... + π̂ʟ + λ * max(0, L - k + 1) - τ ≤ U * π̂ʟ&#39;,</span>

<span class="sd">        otherwise the prediction is {1, ... , L}.</span>

<span class="sd">        This method will be called during prediction when `method=&#39;split&#39;`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y_pred : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            The probability predictions of the test set.</span>

<span class="sd">        tau : float, default=None</span>
<span class="sd">            The generalized inverse quantile of the calibration set.</span>

<span class="sd">        num_threads : int, default=4</span>
<span class="sd">            Number of threads to use for parallel computation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_set_pred : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            Prediction sets of the test data as a binary array where 1</span>
<span class="sd">            indicates class membership in the set.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        .. [1] Yaniv Romano, Matteo Sesia &amp; Emmanuel J. Candès, &quot;Classification</span>
<span class="sd">               with Valid and Adaptive Coverage&quot;, NeurIPS 2020.</span>
<span class="sd">        .. [2] Anastasios Nikolas Angelopoulos, Stephen Bates,</span>
<span class="sd">               Michael I. Jordan &amp; Jitendra Malik, &quot;Uncertainty Sets for Image</span>
<span class="sd">               Classifiers using Conformal Prediction&quot;, ICLR 2021.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">y_set_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">get_state</span><span class="p">()[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">_compute_predictions_split</span><span class="p">(</span>
            <span class="n">y_pred</span><span class="p">,</span>
            <span class="n">y_set_pred</span><span class="p">,</span>
            <span class="n">tau</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k_star_</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_star_</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">randomized</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">allow_empty_sets</span><span class="p">,</span>
            <span class="n">num_threads</span><span class="p">,</span>
            <span class="n">seed</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">y_set_pred</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">binary_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict class labels and prediction sets for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples to predict.</span>

<span class="sd">        alpha : float or None, default=None</span>
<span class="sd">            Desired miscoverage rate. The prediction sets will be constructed</span>
<span class="sd">            to contains the true label with probability at least 1-alpha. If None,</span>
<span class="sd">            only returns y_pred.</span>

<span class="sd">        binary_output : bool, default=False</span>
<span class="sd">            If True, returns prediction sets as binary arrays where 1 indicates</span>
<span class="sd">            the class is in the set. If False, returns lists of class labels.</span>

<span class="sd">        num_threads : int, default=4</span>
<span class="sd">            Number of threads to use for parallel computation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_pred : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted class labels (point predictions).</span>

<span class="sd">        y_set_pred : list of arrays or ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            If binary_output=False, returns a list where each element contains</span>
<span class="sd">            the classes in the prediction set for that sample.</span>
<span class="sd">            If binary_output=True, returns a binary array of shape</span>
<span class="sd">            (n_samples, n_classes) where 1 indicates class membership in the</span>
<span class="sd">            set.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_default</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="s2">&quot;bootstrap&quot;</span><span class="p">]:</span>
            <span class="n">y_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_cv</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">binary_output</span><span class="p">,</span> <span class="n">num_threads</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">binary_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_predict_cv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">binary_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict using CV+ or Jackknife+-after-Bootstrap method.</span>

<span class="sd">        This method will be called during prediction when `method=&#39;cv&#39;`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples to predict.</span>

<span class="sd">        alpha : float or None, default=None</span>
<span class="sd">            Desired miscoverage rate. The prediction sets will be constructed</span>
<span class="sd">            to contains the true label with probability at least 1-alpha. If None,</span>
<span class="sd">            only returns y_pred.</span>

<span class="sd">        binary_output : bool, default=False</span>
<span class="sd">            If True, returns prediction sets as binary arrays where 1 indicates</span>
<span class="sd">            the class is in the set. If False, returns lists of class labels.</span>

<span class="sd">        num_threads : int, default=4</span>
<span class="sd">            Number of threads to use for parallel computation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_pred : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted class labels (point predictions).</span>

<span class="sd">        y_set_pred : list of arrays or ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            If binary_output=False, returns a list where each element contains</span>
<span class="sd">            the classes in the prediction set for that sample.</span>
<span class="sd">            If binary_output=True, returns a binary array of shape</span>
<span class="sd">            (n_samples, n_classes) where 1 indicates</span>
<span class="sd">            class membership in the set.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">y_pred</span><span class="p">,</span> <span class="n">oob_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_oob_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y_pred</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">test_giqs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_test_giqs</span><span class="p">(</span><span class="n">oob_pred</span><span class="p">,</span> <span class="n">num_threads</span><span class="p">)</span>
            <span class="n">compare_giqs</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_giqs_</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">test_giqs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">compare_giqs</span> <span class="o">=</span> <span class="n">compare_giqs</span> <span class="o">&lt;</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">binary_output</span><span class="p">:</span>
                <span class="n">y_set_pred</span> <span class="o">=</span> <span class="n">compare_giqs</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">y_set_pred</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">compare_giqs</span>
                <span class="p">]</span>

            <span class="k">return</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_set_pred</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_predict_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">binary_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict using split conformal method.</span>

<span class="sd">        This method will be called during prediction when `method=&#39;split&#39;`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples to predict.</span>

<span class="sd">        alpha : float or None, default=None</span>
<span class="sd">            Desired miscoverage rate. The prediction sets will be constructed</span>
<span class="sd">            to contains the true label with probability at least 1-alpha. If None,</span>
<span class="sd">            only returns y_pred.</span>

<span class="sd">        binary_output : bool, default=False</span>
<span class="sd">            If True, returns prediction sets as binary arrays where 1 indicates</span>
<span class="sd">            the class is in the set. If False, returns lists of class labels.</span>

<span class="sd">        num_threads : int, default=4</span>
<span class="sd">            Number of threads to use for parallel computation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_pred : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted class labels (point predictions).</span>

<span class="sd">        y_set_pred : list of arrays or ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            If binary_output=False, returns a list where each element contains</span>
<span class="sd">            the classes in the prediction set for that sample.</span>
<span class="sd">            If binary_output=True, returns a binary array of shape</span>
<span class="sd">            (n_samples, n_classes) where 1 indicates class membership in the</span>
<span class="sd">            set.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">y_pred_proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred_proba</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y_pred</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_giqs_</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;higher&quot;</span><span class="p">)</span>
            <span class="n">y_set_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_from_giqs</span><span class="p">(</span><span class="n">y_pred_proba</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">num_threads</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">binary_output</span><span class="p">:</span>
                <span class="n">y_set_pred</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">y_set_pred</span>
                <span class="p">]</span>

            <span class="k">return</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_set_pred</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ConformalForestRegressor</span><span class="p">(</span>
    <span class="n">ConformalRegressorMixin</span><span class="p">,</span> <span class="n">BaseConformalForest</span><span class="p">,</span> <span class="n">ForestRegressor</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for forest of conformal trees-based regressors.</span>

<span class="sd">    This class extends scikit-learn&#39;s ForestRegressor to implement conformal</span>
<span class="sd">    prediction.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">estimator</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">method</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">alpha_default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_forests_per_fold</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">resample_n_estimators</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
        <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
            <span class="n">n_forests_per_fold</span><span class="o">=</span><span class="n">n_forests_per_fold</span><span class="p">,</span>
            <span class="n">resample_n_estimators</span><span class="o">=</span><span class="n">resample_n_estimators</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_default</span> <span class="o">=</span> <span class="n">alpha_default</span>

    <span class="nd">@_fit_context</span><span class="p">(</span><span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">calib_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit the conformal forest regressor.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Training data.</span>

<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            Target values.</span>

<span class="sd">        calib_size : float, default=0.3</span>
<span class="sd">            Used when method=&#39;split&#39;. The proportion of training samples to use</span>
<span class="sd">            for calibration.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Fitted regressor.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_fit_wrapper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">calib_size</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residuals_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_pred_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict regression values and prediction intervals for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples to predict.</span>

<span class="sd">        alpha : float or None, default=None</span>
<span class="sd">            Desired error rate. The prediction intervals will be constructed to</span>
<span class="sd">            contain the true value with probability at least 1-alpha. If None,</span>
<span class="sd">            only returns y_pred.</span>

<span class="sd">        num_threads : int, default=4</span>
<span class="sd">            Number of threads to use for parallel computation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_pred : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted regression values (point predictions).</span>

<span class="sd">        y_intervals : ndarray of shape (n_samples, 2)</span>
<span class="sd">            Prediction intervals for each sample. First column contains lower</span>
<span class="sd">            bounds, second column contains upper bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_default</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="s2">&quot;bootstrap&quot;</span><span class="p">]:</span>
            <span class="n">y_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_cv</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_threads</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_predict_cv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict using CV+ or Jackknife+-after-Bootstrap method.</span>

<span class="sd">        This method will be called during prediction when `method=&#39;cv&#39;`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples to predict.</span>

<span class="sd">        alpha : float or None, default=None</span>
<span class="sd">            Desired error rate. The prediction intervals will be constructed to</span>
<span class="sd">            contain the true value with probability at least 1-alpha. If None,</span>
<span class="sd">            only returns y_pred.</span>

<span class="sd">        num_threads : int, default=4</span>
<span class="sd">            Number of threads to use for parallel computation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_pred : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted regression values (point predictions).</span>

<span class="sd">        y_intervals : ndarray of shape (n_samples, 2)</span>
<span class="sd">            Prediction intervals [lower, upper] for each sample computed using</span>
<span class="sd">            out-of-bag residuals.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">y_pred</span><span class="p">,</span> <span class="n">oob_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_oob_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y_pred</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q_lo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span>
                <span class="n">oob_pred</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">residuals_</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">q_hi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span>
                <span class="n">oob_pred</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">residuals_</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;higher&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">q_lo</span><span class="p">,</span> <span class="n">q_hi</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_predict_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict using split conformal method.</span>

<span class="sd">        This method will be called during prediction when `method=&#39;split&#39;`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples to predict.</span>

<span class="sd">        alpha : float or None, default=None</span>
<span class="sd">            Desired error rate. The prediction intervals will be constructed to</span>
<span class="sd">            contain the true value with probability at least 1-alpha. If None,</span>
<span class="sd">            only returns y_pred.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_pred : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted regression values (point predictions).</span>

<span class="sd">        y_intervals : ndarray of shape (n_samples, 2)</span>
<span class="sd">            Prediction intervals [lower, upper] for each sample computed using</span>
<span class="sd">            calibration set residuals.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">FastRandomForestRegressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y_pred</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">residuals_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;higher&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">q</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">+</span> <span class="n">q</span><span class="p">])</span>


<div class="viewcode-block" id="CoverForestClassifier">
<a class="viewcode-back" href="../../generated/coverforest.CoverForestClassifier.html#coverforest.CoverForestClassifier">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CoverForestClassifier</span><span class="p">(</span><span class="n">ConformalForestClassifier</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A conformal random forest classifier.</span>

<span class="sd">    This class provides an implementation of conformal random forest for</span>
<span class="sd">    prediction sets that contain the true labels with probability above 1-alpha,</span>
<span class="sd">    where alpha is a user-specified miscoverage rate. The prediction sets are</span>
<span class="sd">    constructed using the Adaptive Prediction Set (APS) method.</span>

<span class="sd">    The class supports three subsampling methods for out-of-sample calibration:</span>

<span class="sd">    - &#39;cv&#39;: Uses K-fold cross-validation to split the training set. This method</span>
<span class="sd">      is referred to as CV+.</span>
<span class="sd">    - &#39;bootstrap&#39;: Uses bootstrap subsampling on the training set. This method</span>
<span class="sd">      is referred to as Jackknife+-after-Bootstrap.</span>
<span class="sd">    - &#39;split&#39;: Uses train-test split on the training set. This method</span>
<span class="sd">      is referred to as split conformal.</span>

<span class="sd">    If there a lot of empty sets returned by the `predict()` method, try increasing</span>
<span class="sd">    the target coverage rate by decreasing the value of `alpha`. The option</span>
<span class="sd">    `allow_empty_sets=False` should be used sparingly.</span>

<span class="sd">    The Jackknife+-after-bootstrap implementation (`method=&#39;bootstrap&#39;`) follows [5]</span>
<span class="sd">    Specifically, before fitting, the number of sub-estimators is resampled from the</span>
<span class="sd">    binomial distribution: Binomial(n_estimators / p, p) where</span>

<span class="sd">        p = 1 / (1 - n_samples)**max_samples.</span>

<span class="sd">    To fit the model with exactly `n_estimators` number of sub-estimators, initiate</span>
<span class="sd">    the model with `resample_n_estimators=False`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : int, default=10</span>
<span class="sd">        The number of `sklearn.tree.DecisionTreeClassifier` in the forest.</span>

<span class="sd">    method : {&#39;cv&#39;, &#39;bootstrap&#39;, &#39;split&#39;}, default=&#39;cv&#39;</span>
<span class="sd">        The conformal prediction method to use:</span>

<span class="sd">        - &#39;cv&#39;: Uses CV+ for conformal prediction</span>
<span class="sd">        - &#39;bootstrap&#39;: Uses Jackknife+-after-Bootstrap</span>
<span class="sd">        - &#39;split&#39;: Uses split conformal prediction</span>

<span class="sd">    cv : int or cross-validation generator, default=5</span>
<span class="sd">        Used when `method=&#39;cv&#39;`. If an integer is provided, then it is the</span>
<span class="sd">        number of folds used. See the module sklearn.model_selection module for</span>
<span class="sd">        the list of possible cross-validation objects.</span>

<span class="sd">    k_init : int or &quot;auto&quot;, default=&quot;auto&quot;</span>
<span class="sd">        Initial value for the parameter k that penalizes any set prediction</span>
<span class="sd">        that contains more than k classes.</span>
<span class="sd">        If &quot;auto&quot;, the value is chosen automatically during fitting.</span>

<span class="sd">    lambda_init : float or &quot;auto&quot;, default=&quot;auto&quot;</span>
<span class="sd">        Initial value for lambda parameter (regularization strength).</span>
<span class="sd">        If &quot;auto&quot;, the value is chosen automatically during fitting.</span>

<span class="sd">    repeat_params_search : bool, default=True</span>
<span class="sd">        Whether to repeat the search for optimal parameters when refitting.</span>

<span class="sd">    allow_empty_sets : bool, default=True</span>
<span class="sd">        If True, allows empty prediction sets when no class meets the</span>
<span class="sd">        confidence threshold.</span>

<span class="sd">    randomized : bool, default=True</span>
<span class="sd">        If True, adds randomization during the label selection which yields</span>
<span class="sd">        smaller prediction sets. If False, the predictions will have more</span>
<span class="sd">        conservative coverage.</span>

<span class="sd">    alpha_default : float, default=None</span>
<span class="sd">        The default value of miscoverage rate `alpha` that will be passed to</span>
<span class="sd">        `predict()` whenever it is called indirectly i.e. via scikit-learn&#39;s</span>
<span class="sd">        `GridSearchCV`.</span>

<span class="sd">    n_forests_per_fold : int, default=1</span>
<span class="sd">        Used when `method=&#39;cv&#39;`. The number of the forests to be fitted on each</span>
<span class="sd">        combination of K-1 folds.</span>

<span class="sd">    resample_n_estimators : bool, default=True</span>
<span class="sd">        Used when `method=&#39;bootstrap&#39;`. If True, resample the value of</span>
<span class="sd">        `n_estimators` following the procedure in Kim, Xu &amp; Barber (2020).</span>
<span class="sd">        Specifically, a new number of estimators is sampled from</span>
<span class="sd">        Binomial(n_estimators / p, p) where</span>
<span class="sd">        p = 1 / (1 - n_samples)**max_samples.</span>

<span class="sd">    bootstrap : bool, default=True</span>
<span class="sd">        Whether bootstrap samples are used when building trees. If False, the</span>
<span class="sd">        whole dataset is used to build each tree.</span>
<span class="sd">        When `method=&#39;cv&#39;`, the value will be passed on to the</span>
<span class="sd">        `FastRandomForestClassifier` sub-estimators.</span>

<span class="sd">    max_samples : int or float, default=None</span>
<span class="sd">        If bootstrap is True, the number of samples to draw from X</span>
<span class="sd">        to train each base estimator.</span>

<span class="sd">        - If None (default), then draw `X.shape[0]` samples.</span>
<span class="sd">        - If int, then draw `max_samples` samples.</span>
<span class="sd">        - If float, then draw `max(round(n_samples * max_samples), 1)` samples.</span>

<span class="sd">        Thus, `max_samples` should be in the interval `(0.0, 1.0]`.</span>
<span class="sd">        When `method=&#39;cv&#39;`, the value will be passed on to the</span>
<span class="sd">        `FastRandomForestClassifier` sub-estimators.</span>

<span class="sd">    criterion : {&quot;gini&quot;, &quot;entropy&quot;, &quot;log_loss&quot;}, default=&quot;gini&quot;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria are</span>
<span class="sd">        &quot;gini&quot; for the Gini impurity and &quot;log_loss&quot; and &quot;entropy&quot; both for the</span>
<span class="sd">        Shannon information gain, see `tree_mathematical_formulation`.</span>
<span class="sd">        Note: This parameter is tree-specific.</span>

<span class="sd">    max_depth : int, default=None</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : {&quot;sqrt&quot;, &quot;log2&quot;, None}, int or float, default=&quot;sqrt&quot;</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `max(1, int(max_features * n_features_in_))` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">    oob_score : bool or callable, default=False</span>
<span class="sd">        Whether to use out-of-bag samples to estimate the generalization score.</span>
<span class="sd">        By default, `sklearn.metrics.accuracy_score` is used.</span>
<span class="sd">        Provide a callable with signature `metric(y_true, y_pred)` to use a</span>
<span class="sd">        custom metric. Only available if `bootstrap=True`.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        The number of jobs to run in parallel. `fit`, `predict`,</span>
<span class="sd">        `decision_path` and `apply` are all parallelized over the trees.</span>
<span class="sd">        ``None`` means 1 unless in a `joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors.</span>

<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Controls both the randomness of the bootstrapping of the samples used</span>
<span class="sd">        when building trees (if ``bootstrap=True``) and the sampling of the</span>
<span class="sd">        features to consider when looking for the best split at each node</span>
<span class="sd">        (if ``max_features &lt; n_features``).</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest.</span>

<span class="sd">    class_weight : {&quot;balanced&quot;, &quot;balanced_subsample&quot;}, dict or list of dicts, \</span>
<span class="sd">            default=None</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one. For</span>
<span class="sd">        multi-output problems, a list of dicts can be provided in the same</span>
<span class="sd">        order as the columns of y.</span>

<span class="sd">        Note that for multioutput (including multilabel) weights should be</span>
<span class="sd">        defined for each class of every column in its own dict. For example,</span>
<span class="sd">        for four-class multilabel classification weights should be</span>
<span class="sd">        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of</span>
<span class="sd">        [{1:1}, {2:5}, {3:1}, {4:1}].</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">        The &quot;balanced_subsample&quot; mode is the same as &quot;balanced&quot; except that</span>
<span class="sd">        weights are computed based on the bootstrap sample for every tree</span>
<span class="sd">        grown.</span>

<span class="sd">        For multi-output, the weights of each column of y will be multiplied.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed.</span>

<span class="sd">    monotonic_cst : array-like of int of shape (n_features), default=None</span>
<span class="sd">        Indicates the monotonicity constraint to enforce on each feature.</span>

<span class="sd">          - 1: monotonic increase</span>
<span class="sd">          - 0: no constraint</span>
<span class="sd">          - -1: monotonic decrease</span>

<span class="sd">        If monotonic_cst is None, no constraints are applied.</span>

<span class="sd">        Monotonicity constraints are not supported for classifications trained</span>
<span class="sd">        on data with missing values.</span>

<span class="sd">        The constraints hold over the probability of the positive class.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_ : `FastRandomForestClassifier` or `sklearn.tree.DecisionTreeClassifier`</span>
<span class="sd">        The child estimator template used to create the collection of fitted</span>
<span class="sd">        sub-estimators. It will be a `FastRandomForestClassifier` if `method=&#39;cv&#39;`</span>
<span class="sd">        and `sklearn.tree.DecisionTreeClassifier` otherwise.</span>

<span class="sd">    estimators_ : list of `FastRandomForestClassifier` or \</span>
<span class="sd">    `sklearn.tree.DecisionTreeClassifier`</span>
<span class="sd">        The collection of fitted sub-estimators. A list of</span>
<span class="sd">        `FastRandomForestClassifier` if `method=&#39;cv&#39;` and a list of</span>
<span class="sd">        `sklearn.tree.DecisionTreeClassifier` otherwise.</span>

<span class="sd">    k_star_ : int</span>
<span class="sd">        The optimal k parameter found during fitting.</span>

<span class="sd">    lambda_star_ : float</span>
<span class="sd">        The optimal lambda parameter found during fitting.</span>

<span class="sd">    oob_pred_ : ndarray of shape (n_samples, n_classes, 1)</span>
<span class="sd">        The out-of-bag probability predictions on the training set.</span>

<span class="sd">    train_giqs_ : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        The generalized inverse quantile scores of the training set.</span>

<span class="sd">    classes_ : ndarray of shape (n_classes,) or a list of such arrays</span>
<span class="sd">        The classes labels (single output problem), or a list of arrays of</span>
<span class="sd">        class labels (multi-output problem).</span>

<span class="sd">    n_classes_ : int or list</span>
<span class="sd">        The number of classes (single output problem), or a list containing the</span>
<span class="sd">        number of classes for each output (multi-output problem).</span>

<span class="sd">    n_features_in_ : int</span>
<span class="sd">        Number of features seen during `fit`.</span>

<span class="sd">    feature_names_in_ : ndarray of shape (`n_features_in_`,)</span>
<span class="sd">        Names of features seen during `fit`. Defined only when `X` has feature</span>
<span class="sd">        names that are all strings.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values).</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>
<span class="sd">        This attribute exists only when ``oob_score`` is True.</span>

<span class="sd">    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \</span>
<span class="sd">            (n_samples, n_classes, n_outputs)</span>
<span class="sd">        Decision function computed with out-of-bag estimate on the training</span>
<span class="sd">        set. If n_estimators is small it might be possible that a data point</span>
<span class="sd">        was never left out during the bootstrap. In this case,</span>
<span class="sd">        `oob_decision_function_` might contain NaN. This attribute exists</span>
<span class="sd">        only when ``oob_score`` is True.</span>

<span class="sd">    estimators_samples_ : list of arrays</span>
<span class="sd">        The subset of drawn samples (i.e., the in-bag samples) for each base</span>
<span class="sd">        estimator. Each subset is defined by an array of the indices selected.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    CoverForestRegressor : A conformal random forest for regression tasks.</span>
<span class="sd">    sklearn.ensemble.RandomForestClassifier : The standard random forest</span>
<span class="sd">        classifier from scikit-learn.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Leo Breiman, &quot;Random Forests&quot;, Machine Learning, 45(1), 5-32, 2001.</span>
<span class="sd">    .. [2] Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin &amp; Alexander</span>
<span class="sd">           Gammerman, &quot;Cross-conformal predictive distributions&quot;, 37-51,</span>
<span class="sd">           COPA 2018.</span>
<span class="sd">    .. [3] Yaniv Romano, Matteo Sesia &amp; Emmanuel J. Candès, &quot;Classification with</span>
<span class="sd">           Valid and Adaptive Coverage&quot;, NeurIPS 2020.</span>
<span class="sd">    .. [4] Anastasios Nikolas Angelopoulos, Stephen Bates, Michael I. Jordan &amp;</span>
<span class="sd">           Jitendra Malik, &quot;Uncertainty Sets for Image Classifiers using</span>
<span class="sd">           Conformal Prediction&quot;, ICLR 2021.</span>
<span class="sd">    .. [5] Byol Kim, Chen Xu &amp; Rina Foygel Barber, &quot;Predictive inference is free</span>
<span class="sd">           with the jackknife+-after-bootstrap&quot;, NeurIPS 2020.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from coverforest import CoverForestClassifier</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_classification</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_classification(n_samples=200, n_features=4,</span>
<span class="sd">    ...                           n_informative=2, n_redundant=0,</span>
<span class="sd">    ...                           random_state=0, shuffle=False)</span>
<span class="sd">    &gt;&gt;&gt; clf = CoverForestClassifier(n_estimators=10, method=&#39;cv&#39;, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y)</span>
<span class="sd">    CoverForestClassifier(...)</span>
<span class="sd">    &gt;&gt;&gt; print(clf.predict(X[:1]))</span>
<span class="sd">    (array([0]), [array([0, 1])])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_parameter_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">RandomForestClassifier</span><span class="o">.</span><span class="n">_parameter_constraints</span><span class="p">,</span>
        <span class="o">**</span><span class="n">DecisionTreeClassifier</span><span class="o">.</span><span class="n">_parameter_constraints</span><span class="p">,</span>
        <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;bootstrap&quot;</span><span class="p">,</span> <span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="s2">&quot;split&quot;</span><span class="p">})],</span>
        <span class="s2">&quot;cv&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">),</span> <span class="s2">&quot;cv_object&quot;</span><span class="p">],</span>
        <span class="s2">&quot;k_init&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;auto&quot;</span><span class="p">}),</span> <span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;lambda_init&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;auto&quot;</span><span class="p">}),</span>
            <span class="n">Interval</span><span class="p">(</span><span class="n">Real</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">),</span>
            <span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">),</span>
        <span class="p">],</span>
        <span class="s2">&quot;repeat_params_search&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;boolean&quot;</span><span class="p">],</span>
        <span class="s2">&quot;allow_empty_sets&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;boolean&quot;</span><span class="p">],</span>
        <span class="s2">&quot;randomized&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;boolean&quot;</span><span class="p">],</span>
        <span class="s2">&quot;alpha_default&quot;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Interval</span><span class="p">(</span><span class="n">Real</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;resample_n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;boolean&quot;</span><span class="p">],</span>
        <span class="s2">&quot;class_weight&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;balanced_subsample&quot;</span><span class="p">,</span> <span class="s2">&quot;balanced&quot;</span><span class="p">}),</span>
            <span class="nb">dict</span><span class="p">,</span>
            <span class="nb">list</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
    <span class="n">_parameter_constraints</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;splitter&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">method</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">k_init</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">lambda_init</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">repeat_params_search</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">allow_empty_sets</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">randomized</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">alpha_default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_forests_per_fold</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">resample_n_estimators</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;gini&quot;</span><span class="p">,</span>
        <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;sqrt&quot;</span><span class="p">,</span>
        <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">monotonic_cst</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">estimator_params</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;criterion&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max_features&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span>
            <span class="s2">&quot;random_state&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ccp_alpha&quot;</span><span class="p">,</span>
            <span class="s2">&quot;monotonic_cst&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;cv&quot;</span><span class="p">:</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="n">FastRandomForestClassifier</span><span class="p">(</span>
                <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
                <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
                <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span>
                <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
                <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
                <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
                <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
                <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
                <span class="n">ccp_alpha</span><span class="o">=</span><span class="n">ccp_alpha</span><span class="p">,</span>
                <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
                <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
                <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
                <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">,</span>
                <span class="n">monotonic_cst</span><span class="o">=</span><span class="n">monotonic_cst</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">estimator_params</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="s2">&quot;bootstrap&quot;</span><span class="p">,</span>
                <span class="s2">&quot;max_samples&quot;</span><span class="p">,</span>
                <span class="s2">&quot;n_jobs&quot;</span><span class="p">,</span>
                <span class="s2">&quot;oob_score&quot;</span><span class="p">,</span>
                <span class="s2">&quot;warm_start&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
            <span class="n">k_init</span><span class="o">=</span><span class="n">k_init</span><span class="p">,</span>
            <span class="n">lambda_init</span><span class="o">=</span><span class="n">lambda_init</span><span class="p">,</span>
            <span class="n">repeat_params_search</span><span class="o">=</span><span class="n">repeat_params_search</span><span class="p">,</span>
            <span class="n">allow_empty_sets</span><span class="o">=</span><span class="n">allow_empty_sets</span><span class="p">,</span>
            <span class="n">randomized</span><span class="o">=</span><span class="n">randomized</span><span class="p">,</span>
            <span class="n">alpha_default</span><span class="o">=</span><span class="n">alpha_default</span><span class="p">,</span>
            <span class="n">n_forests_per_fold</span><span class="o">=</span><span class="n">n_forests_per_fold</span><span class="p">,</span>
            <span class="n">resample_n_estimators</span><span class="o">=</span><span class="n">resample_n_estimators</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span> <span class="o">=</span> <span class="n">bootstrap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monotonic_cst</span> <span class="o">=</span> <span class="n">monotonic_cst</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span> <span class="o">=</span> <span class="n">ccp_alpha</span></div>



<div class="viewcode-block" id="CoverForestRegressor">
<a class="viewcode-back" href="../../generated/coverforest.CoverForestRegressor.html#coverforest.CoverForestRegressor">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CoverForestRegressor</span><span class="p">(</span><span class="n">ConformalForestRegressor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A conformal random forest regressor.</span>

<span class="sd">    This class provides an implementation of conformal random forest for</span>
<span class="sd">    prediction intervals that contain the true target value with probability</span>
<span class="sd">    above 1-alpha, where alpha is a user-specified error rate.</span>

<span class="sd">    The class supports three subsampling methods for out-of-sample calibration:</span>

<span class="sd">    - &#39;cv&#39;: Uses K-fold cross-validation to split the training set. This method</span>
<span class="sd">      is referred to as CV+.</span>
<span class="sd">    - &#39;bootstrap&#39;: Uses bootstrap subsampling on the training set. This method</span>
<span class="sd">      is referred to as Jackknife+-after-Bootstrap.</span>
<span class="sd">    - &#39;split&#39;: Uses train-test split on the training set. This method</span>
<span class="sd">      is referred to as split conformal.</span>

<span class="sd">    The Jackknife+-after-bootstrap implementation (`method=&#39;bootstrap&#39;`) follows [5]</span>
<span class="sd">    Specifically, before fitting, the number of sub-estimators is resampled from the</span>
<span class="sd">    binomial distribution: Binomial(n_estimators / p, p) where</span>

<span class="sd">        p = 1 / (1 - n_samples)**max_samples.</span>

<span class="sd">    To fit the model with exactly `n_estimators` number of sub-estimators, initiate</span>
<span class="sd">    the model with `resample_n_estimators=False`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : int, default=10</span>
<span class="sd">        The number of `sklearn.tree.DecisionTreeClassifier` in the forest.</span>

<span class="sd">    method : {&#39;cv&#39;, &#39;bootstrap&#39;, &#39;split&#39;}, default=&#39;cv&#39;</span>
<span class="sd">        The conformal prediction method to use:</span>

<span class="sd">        - &#39;cv&#39;: Uses CV+ for conformal prediction</span>
<span class="sd">        - &#39;bootstrap&#39;: Uses Jackknife+-after-Bootstrap</span>
<span class="sd">        - &#39;split&#39;: Uses split conformal prediction</span>

<span class="sd">    cv : int or cross-validation generator, default=5</span>
<span class="sd">        Used when `method=&#39;cv&#39;`. If an integer is provided, then it is the</span>
<span class="sd">        number of folds used. See the module sklearn.model_selection module for</span>
<span class="sd">        the list of possible cross-validation objects.</span>

<span class="sd">    alpha_default : float, default=None</span>
<span class="sd">        The default value of miscoverage rate `alpha` that will be passed to</span>
<span class="sd">        `predict()` whenever it is called indirectly i.e. via scikit-learn&#39;s</span>
<span class="sd">        `GridSearchCV`.</span>

<span class="sd">    n_forests_per_fold : int, default=1</span>
<span class="sd">        Used when `method=&#39;cv&#39;`. The number of the forests to be fitted on each</span>
<span class="sd">        combination of K-1 folds.</span>

<span class="sd">    resample_n_estimators : bool, default=True</span>
<span class="sd">        Used when `method=&#39;bootstrap&#39;`. If True, resample the value of</span>
<span class="sd">        `n_estimators` following the procedure in Kim, Xu &amp; Barber (2020).</span>
<span class="sd">        Specifically, a new number of estimators is sampled from</span>
<span class="sd">        Binomial(n_estimators / p, p) where</span>
<span class="sd">        p = 1 / (1 - n_samples)**max_samples.</span>

<span class="sd">    criterion : {&quot;squared_error&quot;, &quot;absolute_error&quot;, &quot;friedman_mse&quot;, &quot;poisson&quot;}, \</span>
<span class="sd">            default=&quot;squared_error&quot;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;squared_error&quot; for the mean squared error, which is equal to</span>
<span class="sd">        variance reduction as feature selection criterion and minimizes the L2</span>
<span class="sd">        loss using the mean of each terminal node, &quot;friedman_mse&quot;, which uses</span>
<span class="sd">        mean squared error with Friedman&#39;s improvement score for potential</span>
<span class="sd">        splits, &quot;absolute_error&quot; for the mean absolute error, which minimizes</span>
<span class="sd">        the L1 loss using the median of each terminal node, and &quot;poisson&quot; which</span>
<span class="sd">        uses reduction in Poisson deviance to find splits.</span>
<span class="sd">        Training using &quot;absolute_error&quot; is significantly slower</span>
<span class="sd">        than when using &quot;squared_error&quot;.</span>

<span class="sd">    max_depth : int, default=None</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum number of</span>
<span class="sd">          samples for each node.</span>

<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : {&quot;sqrt&quot;, &quot;log2&quot;, None}, int or float, default=1.0</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `max(1, int(max_features * n_features_in_))` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None or 1.0, then `max_features=n_features`.</span>

<span class="sd">        .. note::</span>
<span class="sd">            The default of 1.0 is equivalent to bagged trees and more</span>
<span class="sd">            randomness can be achieved by setting smaller values, e.g. 0.3.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">    max_samples : int or float, default=None</span>
<span class="sd">        If bootstrap is True, the number of samples to draw from X</span>
<span class="sd">        to train each base estimator.</span>

<span class="sd">        - If None (default), then draw `X.shape[0]` samples.</span>
<span class="sd">        - If int, then draw `max_samples` samples.</span>
<span class="sd">        - If float, then draw `max(round(n_samples * max_samples), 1)` samples.</span>

<span class="sd">        Thus, `max_samples` should be in the interval `(0.0, 1.0]`.</span>
<span class="sd">        When `method=&#39;cv&#39;`, the value will be passed on to the</span>
<span class="sd">        `FastRandomForestClassifier` sub-estimators.</span>

<span class="sd">    bootstrap : bool, default=True</span>
<span class="sd">        Whether bootstrap samples are used when building trees. If False, the</span>
<span class="sd">        whole dataset is used to build each tree.</span>
<span class="sd">        When `method=&#39;cv&#39;`, the value will be passed on to the</span>
<span class="sd">        `FastRandomForestClassifier` sub-estimators.</span>

<span class="sd">    oob_score : bool or callable, default=False</span>
<span class="sd">        Whether to use out-of-bag samples to estimate the generalization score.</span>
<span class="sd">        By default, `sklearn.metrics.r2_score` is used.</span>
<span class="sd">        Provide a callable with signature `metric(y_true, y_pred)` to use a</span>
<span class="sd">        custom metric. Only available if `bootstrap=True`.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        The number of jobs to run in parallel. `fit`, `predict`,</span>
<span class="sd">        `decision_path` and `apply` are all parallelized over the trees.</span>
<span class="sd">        ``None`` means 1 unless in a `joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors.</span>

<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Controls both the randomness of the bootstrapping of the samples used</span>
<span class="sd">        when building trees (if ``bootstrap=True``) and the sampling of the</span>
<span class="sd">        features to consider when looking for the best split at each node</span>
<span class="sd">        (if ``max_features &lt; n_features``).</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest.</span>

<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed.</span>

<span class="sd">    monotonic_cst : array-like of int of shape (n_features), default=None</span>
<span class="sd">        Indicates the monotonicity constraint to enforce on each feature.</span>

<span class="sd">          - 1: monotonically increasing</span>
<span class="sd">          - 0: no constraint</span>
<span class="sd">          - -1: monotonically decreasing</span>

<span class="sd">        If monotonic_cst is None, no constraints are applied.</span>

<span class="sd">        Monotonicity constraints are not supported for regressions trained on</span>
<span class="sd">        data with missing values.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_ : `FastRandomForestClassifier` or `sklearn.tree.DecisionTreeRegressor`</span>
<span class="sd">        The child estimator template used to create the collection of fitted</span>
<span class="sd">        sub-estimators. It will be a `FastRandomForestClassifier` if `method=&#39;cv&#39;`</span>
<span class="sd">        and `sklearn.tree.DecisionTreeRegressor` otherwise.</span>

<span class="sd">    estimators_ : list of `FastRandomForestRegressor` or \</span>
<span class="sd">    `sklearn.tree.DecisionTreeRegressor`</span>
<span class="sd">        The collection of fitted sub-estimators. A list of</span>
<span class="sd">        `FastRandomForestClassifier` if `method=&#39;cv&#39;` and a list of</span>
<span class="sd">        `sklearn.tree.DecisionTreeRegressor` otherwise.</span>

<span class="sd">    oob_pred_ : ndarray of shape (n_samples,)</span>
<span class="sd">        The out-of-bag predictions on the training data.</span>

<span class="sd">    residuals_ : ndarray of shape (n_samples,)</span>
<span class="sd">        The out-of-bag residuals on the training data.</span>

<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>

<span class="sd">    n_features_in_ : int</span>
<span class="sd">        Number of features seen during :term:`fit`.</span>

<span class="sd">    feature_names_in_ : ndarray of shape (`n_features_in_`,)</span>
<span class="sd">        Names of features seen during :term:`fit`. Defined only when `X`</span>
<span class="sd">        has feature names that are all strings.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>
<span class="sd">        This attribute exists only when ``oob_score`` is True.</span>

<span class="sd">    oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">        Prediction computed with out-of-bag estimate on the training set.</span>
<span class="sd">        This attribute exists only when ``oob_score`` is True.</span>

<span class="sd">    estimators_samples_ : list of arrays</span>
<span class="sd">        The subset of drawn samples (i.e., the in-bag samples) for each base</span>
<span class="sd">        estimator. Each subset is defined by an array of the indices selected.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    CoverForestClassifier : A conformal random forest for classification tasks.</span>
<span class="sd">    sklearn.ensemble.RandomForestRegressor : The standard random forest</span>
<span class="sd">        regressor from scikit-learn.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The conformal prediction with K-fold cross-validation (CV+) method was</span>
<span class="sd">    proposed by Romano, Sesia &amp; Candès (2020). The conformal prediction with</span>
<span class="sd">    bootstrap subsampling (Jackknife+-after-Bootstrap) was proposed by Kim, Xu</span>
<span class="sd">    &amp; Barber (2020).</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Leo Breiman, &quot;Random Forests&quot;, Machine Learning, 45(1), 5-32, 2001.</span>
<span class="sd">    .. [2] Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin &amp; Alexander</span>
<span class="sd">           Gammerman, &quot;Cross-conformal predictive distributions&quot;, 37-51,</span>
<span class="sd">           COPA 2018.</span>
<span class="sd">    .. [3] Rina Foygel Barber, Emmanuel J. Candès, Aaditya Ramdas &amp;</span>
<span class="sd">           Ryan J. Tibshirani, &quot;Predictive inference with the jackknife+&quot;,</span>
<span class="sd">           Ann. Statist. 49 (1) 486-507, 2021.</span>
<span class="sd">    .. [4] Byol Kim, Chen Xu, Rina Foygel Barber, &quot;Predictive inference is free</span>
<span class="sd">           with the jackknife+-after-bootstrap&quot;, NeurIPS 2020.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from coverforest import CoverForestRegressor</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_regression</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_regression(n_samples=200, n_features=4, n_informative=2,</span>
<span class="sd">    ...                        random_state=0, shuffle=False)</span>
<span class="sd">    &gt;&gt;&gt; regr = CoverForestRegressor(n_estimators=10, method=&#39;cv&#39;, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; regr.fit(X, y)</span>
<span class="sd">    CoverForestRegressor(...)</span>
<span class="sd">    &gt;&gt;&gt; print(regr.predict([[0, 0, 0, 0]]))</span>
<span class="sd">    (array([13.14530751]), array([[-44.87496872,  73.12034704]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_parameter_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">RandomForestRegressor</span><span class="o">.</span><span class="n">_parameter_constraints</span><span class="p">,</span>
        <span class="o">**</span><span class="n">DecisionTreeRegressor</span><span class="o">.</span><span class="n">_parameter_constraints</span><span class="p">,</span>
        <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;bootstrap&quot;</span><span class="p">,</span> <span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="s2">&quot;split&quot;</span><span class="p">})],</span>
        <span class="s2">&quot;cv&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">),</span> <span class="s2">&quot;cv_object&quot;</span><span class="p">],</span>
        <span class="s2">&quot;alpha_default&quot;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Interval</span><span class="p">(</span><span class="n">Real</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;resample_n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;boolean&quot;</span><span class="p">],</span>
    <span class="p">}</span>
    <span class="n">_parameter_constraints</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;splitter&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">method</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">alpha_default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_forests_per_fold</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">resample_n_estimators</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;squared_error&quot;</span><span class="p">,</span>
        <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;sqrt&quot;</span><span class="p">,</span>
        <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">monotonic_cst</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">estimator_params</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;criterion&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max_features&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span>
            <span class="s2">&quot;random_state&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ccp_alpha&quot;</span><span class="p">,</span>
            <span class="s2">&quot;monotonic_cst&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;cv&quot;</span><span class="p">:</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="n">FastRandomForestRegressor</span><span class="p">(</span>
                <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
                <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
                <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span>
                <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
                <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
                <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
                <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
                <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
                <span class="n">ccp_alpha</span><span class="o">=</span><span class="n">ccp_alpha</span><span class="p">,</span>
                <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
                <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
                <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
                <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">,</span>
                <span class="n">monotonic_cst</span><span class="o">=</span><span class="n">monotonic_cst</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">estimator_params</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="s2">&quot;bootstrap&quot;</span><span class="p">,</span>
                <span class="s2">&quot;max_samples&quot;</span><span class="p">,</span>
                <span class="s2">&quot;n_jobs&quot;</span><span class="p">,</span>
                <span class="s2">&quot;oob_score&quot;</span><span class="p">,</span>
                <span class="s2">&quot;warm_start&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
            <span class="n">alpha_default</span><span class="o">=</span><span class="n">alpha_default</span><span class="p">,</span>
            <span class="n">n_forests_per_fold</span><span class="o">=</span><span class="n">n_forests_per_fold</span><span class="p">,</span>
            <span class="n">resample_n_estimators</span><span class="o">=</span><span class="n">resample_n_estimators</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span> <span class="o">=</span> <span class="n">bootstrap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monotonic_cst</span> <span class="o">=</span> <span class="n">monotonic_cst</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span> <span class="o">=</span> <span class="n">ccp_alpha</span></div>

</pre></div>

                </article>






                <footer class="prev-next-footer d-print-none">

<div class="prev-next-area">
</div>
                </footer>

            </div>




          </div>
          <footer class="bd-footer-content">

<div class="bd-footer-content__inner container">

  <div class="footer-item">

<p class="component-author">
By D. Ponnoprat
</p>

  </div>

  <div class="footer-item">


  <p class="copyright">

      © Copyright 2025, D. Ponnoprat.
      <br/>

  </p>

  </div>

  <div class="footer-item">

  </div>

  <div class="footer-item">

  </div>

</div>
          </footer>


      </main>
    </div>
  </div>

  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
